services:
  # Ollama LLM Server with WSL2/GTX compatibility fixes
  ollama:
    image: ollama/ollama:0.11.10
    container_name: ape_ollama
    ports:
      - "11434:11434"
    volumes:
      - /mnt/e/ollama_models:/root/.ollama/models
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - NVIDIA_VISIBLE_DEVICES=0,1
      - OLLAMA_GPU_LAYERS=-1
      - OLLAMA_GPU_MEMORY_FRACTION=0.9
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # APE MCP Server
  mcp-server:
    build:
      context: .
      dockerfile: Dockerfile
    platform: linux/amd64
    image: ape-mcp-server # Added image name
    container_name: ape_mcp_server
    command: ["python", "mcp_server.py"]
    ports:
      - "8000:8000"
    volumes:
      - ape_data:/app/data
      - ape_db:/app/database
    env_file:
      - .env
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - SESSION_DB_PATH=/app/database/sessions.db
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; socket.create_connection(('localhost', 8000))"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # APE CLI Agent (Interactive)
  agent:
    build:
      context: .
      dockerfile: Dockerfile
    platform: linux/amd64
    image: ape-agent # Added unique image name
    container_name: ape_agent
    command: ["python", "cli_chat.py"]
    volumes:
      - ./logs:/app/logs
      - ape_data:/app/data
      - ape_db:/app/database
    env_file:
      - .env
    environment:
      - MCP_SERVER_URL=http://mcp-server:8000
      - OLLAMA_BASE_URL=http://ollama:11434
      - SESSION_DB_PATH=/app/database/sessions.db
      - HF_HOME=/app/data/.cache
      - TERM=xterm-256color
    depends_on:
      mcp-server:
        condition: service_healthy
      ollama:
        condition: service_healthy
    stdin_open: true    # Keep stdin open for interactive mode
    tty: true          # Allocate a pseudo-TTY for proper terminal
    restart: "no"      # Don't restart agent automatically
    profiles:
      - interactive    # Only start when explicitly requested

volumes:
  ollama_data:
    driver: local
  ape_data:
    driver: local
  ape_db:
    driver: local

networks:
  default:
    name: ape_network
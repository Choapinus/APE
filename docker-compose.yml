services:
  # Ollama LLM Server with WSL2/GTX compatibility fixes
  ollama:
    image: ollama/ollama:0.9.2
    container_name: ape_ollama
    ports:
      - "11434:11434"
    volumes:
      - /mnt/e/ollama_models:/root/.ollama/models
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_ORIGINS=*
      - OLLAMA_MODELS=/root/.ollama/models
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_RUNNERS_DIR=/tmp/ollama_runners
      - OLLAMA_DEBUG=1
      - CUDA_VISIBLE_DEVICES=1
      - OLLAMA_GPU_OVERHEAD=4096
      - OLLAMA_MAX_VRAM=6144
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=false
      - OLLAMA_USE_MLOCK=false
      - CUDA_LAUNCH_BLOCKING=1
      - CUDA_MEMORY_POOL_DISABLED=1
      - OLLAMA_LOAD_TIMEOUT=600
      - OLLAMA_REQUEST_TIMEOUT=300
    restart: unless-stopped
    deploy:
       resources:
         reservations:
           devices:
             - driver: nvidia
               device_ids: ['1']  # Use GTX 1080 Ti (11GB) for better compatibility
               capabilities: [gpu]
         limits:
           memory: 8G  # Limit container memory
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # APE MCP Server
  mcp-server:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ape_mcp_server
    command: ["sh", "-c", "export MCP_JWT_KEY=$$(openssl rand -hex 32) && python mcp_server.py"]
    ports:
      - "8000:8000"
    volumes:
      - ape_data:/app/data
      - ./ape/sessions.db:/app/ape/sessions.db
    env_file:
      - .env
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - SESSION_DB_PATH=/app/ape/sessions.db
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; socket.create_connection(('localhost', 8000))"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # APE CLI Agent (Interactive)
  agent:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ape_agent
    command: ["sh", "-c", "export MCP_JWT_KEY=$$(openssl rand -hex 32) && python cli_chat.py"]
    volumes:
      - ape_data:/app/data
      - ./ape/sessions.db:/app/ape/sessions.db
    env_file:
      - .env
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - SESSION_DB_PATH=/app/ape/sessions.db
    depends_on:
      mcp-server:
        condition: service_healthy
      ollama:
        condition: service_healthy
    stdin_open: true    # Keep stdin open for interactive mode
    tty: true          # Allocate a pseudo-TTY for proper terminal
    restart: "no"      # Don't restart agent automatically
    profiles:
      - interactive    # Only start when explicitly requested

volumes:
  ollama_data:
    driver: local
  ape_data:
    driver: local

networks:
  default:
    name: ape_network
